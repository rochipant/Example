# -*- coding: utf-8 -*-
"""Copia de Proyecto_Final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SEkuFcATyuTQQWJ_JaVQipRHUePk5rmD

>  **Proyecto final** //
Catalina del Rocio Pantoja Ordoñez

#Librerias
"""

# Importar librerías

import numpy as np
import pandas as pd
from pandas.plotting import scatter_matrix
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn import model_selection
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.neural_network import MLPClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn import tree
from sklearn.svm import SVC
from sklearn.manifold import TSNE
from sklearn.decomposition import PCA

from sklearn import linear_model
from sklearn import datasets
from sklearn import metrics
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import PolynomialFeatures
from sklearn.metrics import confusion_matrix
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor

from sklearn import linear_model
from sklearn import datasets
from sklearn import metrics
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix
from sklearn.metrics import precision_score
from sklearn.metrics import accuracy_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score
from sklearn.metrics import roc_auc_score
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier

from sklearn import linear_model
from sklearn import datasets
from sklearn import metrics
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix
from sklearn.metrics import precision_score
from sklearn.metrics import accuracy_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score
from sklearn.metrics import roc_auc_score
from sklearn.neighbors import KNeighborsClassifier

from sklearn.datasets import make_blobs
from sklearn.cluster import KMeans
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.cluster.hierarchy import dendrogram
from sklearn.datasets import load_iris
from sklearn.cluster import AgglomerativeClustering

"""#Preprocesamiento"""

# Cargar datos
diab=datasets.load_digits()

# Imprimir información Dataset
print(diab.keys())

# Imprimir descripción de datos
print(diab.DESCR)

# Imprimir nombres de columnas
print(diab.feature_names)

#Cargar Informacion en un cuadro de datos
data=diab.data
dataset = pd.DataFrame(data, columns=[diab.feature_names])
dataset

# Tamaño
print(dataset.shape)

# Estadísticas

# Descripción de datos
print(dataset.describe())

# Imprimir nombres de columnas
print(diab.target_names)

A=diab.target
A

A = pd.DataFrame(A, columns=['target'])
A

# Pasan los datos de las clases a la variable y
y=diab.target
y2 = pd.DataFrame(y, columns=['target'])

# Distribución por clases
fin=[dataset,y]
print(dataset.groupby(y).size())

# Descripcion datos estadisticos
print(dataset[diab.feature_names].describe())
plt.figure(figsize=(9, 8))
sns.distplot(dataset[diab.feature_names], color='g', bins=100, hist_kws={'alpha': 0.4});

fin

# Relación entre características

# Correlación
df_num = dataset
df_num.head()
corr = df_num.corr()
plt.figure(figsize=(20, 20))
sns.heatmap(corr[(corr >= 0.001) | (corr <= -0.001)],cmap='viridis',vmax=1.0,vmin=-1.0,linewidths=0.1,annot=True, annot_kws={"size": 10}, square=True);
print( diab.feature_names )

"""#Support Vector Machine"""

# Seleccionar X y Y en base de datos

x = diab.data[:,np.newaxis,1]
y = diab.data[:,np.newaxis,57]

# Graficar

plt.scatter(x,y)
plt.xlabel('mean radius')
plt.ylabel('worst radius')

# Particionar datos
X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)

# Modelo de regresión de soporte vectorial

svr = SVR(kernel='linear',C=1.0,epsilon=0.2)
svr.fit(X_train,y_train)
y_pred = svr.predict(X_test)

# Gráfica de modelo

plt.scatter(X_test,y_test)
plt.plot(X_test,y_pred,color='red',linewidth=3)
plt.title('Regresión Soporte Vectorial')
plt.xlabel('mean radius')
plt.ylabel('worst radius')

# Precisión del modelo
svr.score(X_train,y_train)

"""#Bosque Aleatorio"""

# Seleccionar X y Y en base de datos

x = diab.data
y = diab.target

# Particionar datos
X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)

# Algoritmo
RFC = RandomForestClassifier(n_estimators = 10, criterion = 'entropy')
RFC.fit(X_train, y_train)
y_pred = RFC.predict(X_test)

# Matriz de confusión
matriz = confusion_matrix(y_test,y_pred)
print(matriz)

# Precisión
pres = precision_score(y_test,y_pred)
print(pres)

# Exactitud

acc = accuracy_score(y_test,y_pred)
print(acc)

# Sensibilidad

sens = recall_score(y_test,y_pred)
print(sens)

# Puntaje F1

f1 = f1_score(y_test,y_pred)
print(f1)

# Curva ROC - AUC

AUC = roc_auc_score(y_test,y_pred)
print(AUC)

"""#Arbol de desiciones"""

# Seleccionar X y Y en base de datos

x = diab.data[:,5:8]
y = diab.data[:,3]

# Graficar

columna = x[:,0]
plt.scatter(columna,y)
plt.xlabel('mean compactness--mean symmetry')
plt.ylabel('mean area')

# Particionar datos
X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)

# Modelo de regresión lineal

DTR = DecisionTreeRegressor(max_depth=5)
DTR.fit(X_train,y_train)
Y_pred = DTR.predict(X_test)

# Precisión del modelo
DTR.score(X_train,y_train)

"""#K-Nearest Neighbors"""

# Seleccionar X y Y en base de datos

x = diab.data
y = diab.target

# Particionar datos
X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)

# Algoritmo
KNN = KNeighborsClassifier(n_neighbors=5,metric='minkowski',p=2)
KNN.fit(X_train, y_train)
y_pred = KNN.predict(X_test)

# Matriz de confusión

matriz = confusion_matrix(y_test,y_pred)
print(matriz)

# Exactitud

acc = accuracy_score(y_test,y_pred)
print(acc)

# Sensibilidad

sens = recall_score(y_test,y_pred)
print(sens)

"""#Agrupación jerárquica




"""

dataset.shape

dataset.head()

data = dataset.iloc[:, 3:5].values

import scipy.cluster.hierarchy as shc
plt.figure(figsize=(10, 7))
plt.title("Customer Dendograms")
dend = shc.dendrogram(shc.linkage(data, method='ward'))

from sklearn.cluster import AgglomerativeClustering

cluster = AgglomerativeClustering(n_clusters=5, affinity='euclidean', linkage='ward')
cluster.fit_predict(data)

plt.figure(figsize=(10, 7))
plt.scatter(data[:,0], data[:,1], c=cluster.labels_, cmap='rainbow')

"""#KMeans"""

data=diab.data
print(data.shape)

# Cambiar fondo
data = 255-data

n = 9

kmeans = KMeans(n_clusters=n,init='random')
kmeans.fit(data)
Z = kmeans.predict(data)

for i in range(0,n):

    fila = np.where(Z==i)[0] # filas en Z donde están las imagenes de cada cluster
    num = fila.shape[0]      # numero imagenes de cada cluster
    r = np.floor(num/10.)    # numero de filas menos 1 en figura de salida

    print("cluster "+str(i))
    print(str(num)+" elementos")

    plt.figure(figsize=(10,10))
    for k in range(0, num):
        plt.subplot(r+1, 10, k+1)
        imagen = data[fila[k], ]
        imagen = imagen.reshape(8, 8)
        plt.imshow(imagen, cmap=plt.cm.gray)
        plt.axis('off')
    plt.show()

"""#Agrupacion Particional"""

n = 3
k_means = KMeans(n_clusters=n)
k_means.fit(diab.data)

centroides = k_means.cluster_centers_
etiquetas = k_means.labels_

plt.plot(diab.data[etiquetas==0,2],diab.data[etiquetas==0,3],'r.', label='cluster 1')
plt.plot(diab.data[etiquetas==1,2],diab.data[etiquetas==1,4],'b.', label='cluster 2')
plt.plot(diab.data[etiquetas==2,3],diab.data[etiquetas==2,3],'g.', label='cluster 3')

plt.plot(centroides[:,0],centroides[:,3],'mo',markersize=8, label='centroides')

plt.legend(loc='best')
plt.show()